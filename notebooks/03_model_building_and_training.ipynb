{"cells":[{"cell_type":"code","execution_count":1,"id":"NXUqie6TpAop","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NXUqie6TpAop","executionInfo":{"status":"ok","timestamp":1754814680212,"user_tz":-330,"elapsed":1998,"user":{"displayName":"DEEKSHITHA B","userId":"16421084222432259444"}},"outputId":"3fd53b27-6e60-45c4-d3e3-e82ff83eb6a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"id":"eK33OqalswRD","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13556,"status":"ok","timestamp":1754814693766,"user":{"displayName":"DEEKSHITHA B","userId":"16421084222432259444"},"user_tz":-330},"id":"eK33OqalswRD","outputId":"be0427e3-5a6a-4974-a09f-3eef274b6599"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading the processed DataFrame from Google Drive...\n","DataFrame loaded with shape: (52116, 325)\n","Target data shape: (52116, 325)\n","\n","Normalizing the target data...\n","Data successfully normalized.\n","\n","Sequential data creation complete.\n","Training input sequences shape: (41674, 12, 325)\n","Training output sequences shape: (41674, 6, 325)\n","Validation input sequences shape: (10406, 12, 325)\n","Validation output sequences shape: (10406, 6, 325)\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","import os\n","\n","# --- Define file paths and load the processed data from Google Drive ---\n","print(\"Loading the processed DataFrame from Google Drive...\")\n","base_path = '/content/drive/MyDrive/smart_traffic_system'\n","processed_file_path = os.path.join(base_path, 'traffic_data_processed.parquet')\n","df = pd.read_parquet(processed_file_path)\n","print(f\"DataFrame loaded with shape: {df.shape}\")\n","\n","# --- Separate target data and features ---\n","target_columns = [col for col in df.columns if col.startswith('sensor_')]\n","target_data = df[target_columns].values\n","print(f\"Target data shape: {target_data.shape}\")\n","\n","# --- Split the data into training and validation sets ---\n","train_split = 0.8\n","num_samples = df.shape[0]\n","num_train_samples = int(num_samples * train_split)\n","num_val_samples = num_samples - num_train_samples\n","\n","X_train_targets = target_data[:num_train_samples]\n","X_val_targets = target_data[num_train_samples:]\n","\n","# --- Normalize the data ---\n","print(\"\\nNormalizing the target data...\")\n","scaler = MinMaxScaler()\n","scaler.fit(X_train_targets)\n","\n","X_train_targets_scaled = scaler.transform(X_train_targets)\n","X_val_targets_scaled = scaler.transform(X_val_targets)\n","print(\"Data successfully normalized.\")\n","\n","# --- Create sequential data format for the Transformer ---\n","def create_sequences(data, input_steps, output_steps):\n","    X, y = [], []\n","    for i in range(len(data) - input_steps - output_steps):\n","        X.append(data[i:(i + input_steps)])\n","        y.append(data[(i + input_steps):(i + input_steps + output_steps)])\n","    return np.array(X), np.array(y)\n","\n","input_sequence_length = 12 # 1 hour of history (12 * 5-minute intervals)\n","output_sequence_length = 6 # 30 minutes to predict (6 * 5-minute intervals)\n","\n","X_train_seq, y_train_seq = create_sequences(X_train_targets_scaled, input_sequence_length, output_sequence_length)\n","X_val_seq, y_val_seq = create_sequences(X_val_targets_scaled, input_sequence_length, output_sequence_length)\n","\n","print(\"\\nSequential data creation complete.\")\n","print(f\"Training input sequences shape: {X_train_seq.shape}\")\n","print(f\"Training output sequences shape: {y_train_seq.shape}\")\n","print(f\"Validation input sequences shape: {X_val_seq.shape}\")\n","print(f\"Validation output sequences shape: {y_val_seq.shape}\")"]},{"cell_type":"code","execution_count":3,"id":"6V6MYwb8swHF","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"elapsed":2896,"status":"ok","timestamp":1754814696665,"user":{"displayName":"DEEKSHITHA B","userId":"16421084222432259444"},"user_tz":-330},"id":"6V6MYwb8swHF","outputId":"e1dcc85f-c401-4627-8661-a37970b40820"},"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional_1\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m325\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m32,600\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_block               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │       \u001b[38;5;34m168,232\u001b[0m │\n","│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │         \u001b[38;5;34m2,020\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1950\u001b[0m)           │        \u001b[38;5;34m40,950\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m325\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,600</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_block               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">168,232</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,020</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1950</span>)           │        <span style=\"color: #00af00; text-decoration-color: #00af00\">40,950</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m243,802\u001b[0m (952.35 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">243,802</span> (952.35 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m243,802\u001b[0m (952.35 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">243,802</span> (952.35 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}],"source":["# --- Define the Transformer block ---\n","class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super().__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","# --- Build the full Transformer model ---\n","def build_transformer_model(input_shape, output_seq_len):\n","    embed_dim = 100  # Embedding size for each token\n","    num_heads = 4    # Number of attention heads\n","    ff_dim = 32      # Hidden layer size in feedforward network\n","\n","    inputs = layers.Input(shape=input_shape)\n","\n","    x = layers.Dense(embed_dim)(inputs)\n","\n","    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n","    x = transformer_block(x)\n","\n","    x = layers.GlobalAveragePooling1D()(x)\n","    x = layers.Dropout(0.1)(x)\n","    x = layers.Dense(20, activation=\"relu\")(x)\n","    x = layers.Dropout(0.1)(x)\n","\n","    outputs = layers.Dense(output_seq_len * input_shape[-1])(x)\n","    outputs = layers.Reshape((output_seq_len, input_shape[-1]))(outputs)\n","\n","    model = keras.Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n","model = build_transformer_model(input_shape, output_sequence_length)\n","model.summary()"]},{"cell_type":"code","execution_count":4,"id":"fhxLx1h1sv63","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":454301,"status":"ok","timestamp":1754815150969,"user":{"displayName":"DEEKSHITHA B","userId":"16421084222432259444"},"user_tz":-330},"id":"fhxLx1h1sv63","outputId":"03018133-f85a-4388-8edc-d9382a840ae1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - loss: 0.0874 - val_loss: 0.0075\n","Epoch 2/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.0120 - val_loss: 0.0065\n","Epoch 3/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0103 - val_loss: 0.0064\n","Epoch 4/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0092 - val_loss: 0.0060\n","Epoch 5/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.0081 - val_loss: 0.0059\n","Epoch 6/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0072 - val_loss: 0.0057\n","Epoch 7/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.0064 - val_loss: 0.0053\n","Epoch 8/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0056 - val_loss: 0.0053\n","Epoch 9/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0052 - val_loss: 0.0051\n","Epoch 10/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.0049 - val_loss: 0.0051\n","Epoch 11/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0048 - val_loss: 0.0049\n","Epoch 12/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0046 - val_loss: 0.0050\n","Epoch 13/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0045 - val_loss: 0.0049\n","Epoch 14/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0044 - val_loss: 0.0047\n","Epoch 15/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.0044 - val_loss: 0.0047\n","Epoch 16/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0043 - val_loss: 0.0046\n","Epoch 17/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0042 - val_loss: 0.0047\n","Epoch 18/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0043 - val_loss: 0.0047\n","Epoch 19/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.0042 - val_loss: 0.0045\n","Epoch 20/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0042 - val_loss: 0.0049\n","Epoch 21/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0042 - val_loss: 0.0045\n","Epoch 22/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0042 - val_loss: 0.0045\n","Epoch 23/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0042 - val_loss: 0.0045\n","Epoch 24/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.0041 - val_loss: 0.0048\n","Epoch 25/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0041 - val_loss: 0.0046\n","Epoch 26/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.0041 - val_loss: 0.0045\n","Epoch 27/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0041 - val_loss: 0.0045\n","Epoch 28/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0041 - val_loss: 0.0046\n","Epoch 29/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0041 - val_loss: 0.0046\n","Epoch 30/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0041 - val_loss: 0.0045\n","Epoch 31/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0041 - val_loss: 0.0044\n","Epoch 32/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 0.0041 - val_loss: 0.0044\n","Epoch 33/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0041 - val_loss: 0.0044\n","Epoch 34/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0041 - val_loss: 0.0045\n","Epoch 35/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0040 - val_loss: 0.0044\n","Epoch 36/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0041 - val_loss: 0.0046\n","Epoch 37/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0040 - val_loss: 0.0044\n","Epoch 38/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0040 - val_loss: 0.0046\n","Epoch 39/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0040 - val_loss: 0.0045\n","Epoch 40/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0040 - val_loss: 0.0044\n","Epoch 41/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0040 - val_loss: 0.0044\n","Epoch 42/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0040 - val_loss: 0.0045\n","Epoch 43/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0040 - val_loss: 0.0045\n","Epoch 44/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0040 - val_loss: 0.0045\n","Epoch 45/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0040 - val_loss: 0.0044\n","Epoch 46/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0040 - val_loss: 0.0046\n","Epoch 47/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0040 - val_loss: 0.0044\n","Epoch 48/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0040 - val_loss: 0.0045\n","Epoch 49/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0040 - val_loss: 0.0044\n","Epoch 50/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0040 - val_loss: 0.0044\n","Epoch 51/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0040 - val_loss: 0.0045\n","Epoch 52/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0040 - val_loss: 0.0044\n","Epoch 53/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 54/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.0040 - val_loss: 0.0044\n","Epoch 55/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 56/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0040 - val_loss: 0.0044\n","Epoch 57/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0040 - val_loss: 0.0044\n","Epoch 58/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0040 - val_loss: 0.0044\n","Epoch 59/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.0040 - val_loss: 0.0044\n","Epoch 60/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 61/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 62/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0040 - val_loss: 0.0044\n","Epoch 63/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0040 - val_loss: 0.0044\n","Epoch 64/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 65/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.0039 - val_loss: 0.0046\n","Epoch 66/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 67/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 68/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 69/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0040 - val_loss: 0.0044\n","Epoch 70/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 71/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0040 - val_loss: 0.0044\n","Epoch 72/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0039 - val_loss: 0.0043\n","Epoch 73/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 74/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0043\n","Epoch 75/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0040 - val_loss: 0.0043\n","Epoch 76/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 77/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 78/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 79/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0043\n","Epoch 80/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0039 - val_loss: 0.0043\n","Epoch 81/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0039 - val_loss: 0.0043\n","Epoch 82/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0045\n","Epoch 83/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 84/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 85/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0043\n","Epoch 86/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0043\n","Epoch 87/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 88/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 89/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0043\n","Epoch 90/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.0040 - val_loss: 0.0043\n","Epoch 91/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 92/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0043\n","Epoch 93/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 94/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0039 - val_loss: 0.0043\n","Epoch 95/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0043\n","Epoch 96/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0038 - val_loss: 0.0043\n","Epoch 97/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0039 - val_loss: 0.0043\n","Epoch 98/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0038 - val_loss: 0.0044\n","Epoch 99/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0044\n","Epoch 100/100\n","\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0046\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\n","Model saved to: /content/drive/MyDrive/smart_traffic_system/transformer_model.h5\n"]}],"source":["# --- Compile the model ---\n","model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n","\n","# --- Train the model ---\n","history = model.fit(\n","    X_train_seq,\n","    y_train_seq,\n","    batch_size=64,\n","    epochs=100,\n","    validation_data=(X_val_seq, y_val_seq)\n",")\n","\n","# --- Save the trained model ---\n","model_save_path = os.path.join(base_path, 'transformer_model.h5')\n","model.save(model_save_path)\n","print(f\"\\nModel saved to: {model_save_path}\")"]},{"cell_type":"code","execution_count":5,"id":"GrYv8wX3wJL-","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4881,"status":"ok","timestamp":1754815155871,"user":{"displayName":"DEEKSHITHA B","userId":"16421084222432259444"},"user_tz":-330},"id":"GrYv8wX3wJL-","outputId":"06f83d90-1bdf-4c75-adf4-284ef17249f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading the trained model...\n","Error loading model: Unknown layer: 'TransformerBlock'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n","Making predictions on the validation data...\n","\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step\n","Prediction shape: (10406, 6, 325)\n","\n","Evaluating model performance...\n","Mean Squared Error (MSE): 23.17\n","Root Mean Squared Error (RMSE): 4.81\n","Mean Absolute Error (MAE): 2.56\n","R-squared (R2) Score: 0.6277\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from tensorflow import keras\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","import os\n","\n","# --- Step 15: Load the trained model and make predictions ---\n","print(\"Loading the trained model...\")\n","base_path = '/content/drive/MyDrive/smart_traffic_system'\n","model_path = os.path.join(base_path, 'transformer_model.h5')\n","\n","try:\n","    model = keras.models.load_model(model_path)\n","    print(\"Model loaded successfully.\")\n","except Exception as e:\n","    print(f\"Error loading model: {e}\")\n","    # The model architecture code from Step 15 needs to be in a previous cell\n","    # in the notebook for this to work.\n","\n","# Make predictions on the validation set\n","print(\"Making predictions on the validation data...\")\n","y_pred_scaled = model.predict(X_val_seq)\n","print(f\"Prediction shape: {y_pred_scaled.shape}\")\n","\n","# The predictions are scaled, so we need to inverse-transform them\n","# We will reshape the predictions to a 2D array for inverse transformation\n","num_val_seq = y_val_seq.shape[0]\n","num_output_steps = y_val_seq.shape[1]\n","num_sensors = y_val_seq.shape[2]\n","\n","y_val_seq_flat = y_val_seq.reshape(num_val_seq * num_output_steps, num_sensors)\n","y_pred_scaled_flat = y_pred_scaled.reshape(num_val_seq * num_output_steps, num_sensors)\n","\n","# We need the scaler object to inverse-transform. The scaler was fit on X_train_targets.\n","# We will load the scaler from memory if the notebook is run top-to-bottom.\n","# We need to make sure we have the original scaler object from Step 14.\n","\n","# Inverse transform the true values and the predictions\n","y_val_true = scaler.inverse_transform(y_val_seq_flat)\n","y_val_pred = scaler.inverse_transform(y_pred_scaled_flat)\n","\n","# --- Step 16: Evaluate model performance ---\n","print(\"\\nEvaluating model performance...\")\n","\n","# Calculate Mean Squared Error (MSE)\n","mse = mean_squared_error(y_val_true, y_val_pred)\n","print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n","\n","# Calculate Root Mean Squared Error (RMSE)\n","rmse = np.sqrt(mse)\n","print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n","\n","# Calculate Mean Absolute Error (MAE) - a more intuitive metric\n","mae = np.mean(np.abs(y_val_true - y_val_pred))\n","print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n","\n","# Calculate R-squared (R2) score\n","r2 = r2_score(y_val_true, y_val_pred)\n","print(f\"R-squared (R2) Score: {r2:.4f}\")"]},{"cell_type":"code","execution_count":6,"id":"q69YjEKDwJPK","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3568,"status":"ok","timestamp":1754815159442,"user":{"displayName":"DEEKSHITHA B","userId":"16421084222432259444"},"user_tz":-330},"id":"q69YjEKDwJPK","outputId":"01d132dc-418b-4fb5-868f-934370a54bdc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading the trained model with custom objects...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["Model loaded successfully.\n","\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n","\n","Evaluating model performance...\n","Mean Squared Error (MSE): 23.17\n","Root Mean Squared Error (RMSE): 4.81\n","Mean Absolute Error (MAE): 2.56\n","R-squared (R2) Score: 0.6277\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from tensorflow import keras\n","from tensorflow.keras.utils import custom_object_scope # <-- New import\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler\n","import os\n","\n","# We need to re-define the custom TransformerBlock class so Keras knows what it is.\n","class TransformerBlock(keras.layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n","        super().__init__(**kwargs)\n","        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = keras.Sequential(\n","            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = keras.layers.Dropout(rate)\n","        self.dropout2 = keras.layers.Dropout(rate)\n","        # Store parameters for serialization\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.ff_dim = ff_dim\n","        self.rate = rate\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"ff_dim\": self.ff_dim,\n","            \"rate\": self.rate,\n","        })\n","        return config\n","\n","    def call(self, inputs):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","\n","# --- Step 18: Load the trained model with custom objects ---\n","print(\"Loading the trained model with custom objects...\")\n","base_path = '/content/drive/MyDrive/smart_traffic_system'\n","model_path = os.path.join(base_path, 'transformer_model.h5')\n","\n","with custom_object_scope({'TransformerBlock': TransformerBlock}):\n","    model = keras.models.load_model(model_path)\n","\n","print(\"Model loaded successfully.\")\n","\n","# We will now use the scaler and X_val_seq from the previous cells, so make sure they are in memory\n","\n","# Make predictions on the validation set\n","y_pred_scaled = model.predict(X_val_seq)\n","\n","# --- Inverse transform the true values and the predictions ---\n","num_val_seq = y_val_seq.shape[0]\n","num_output_steps = y_val_seq.shape[1]\n","num_sensors = y_val_seq.shape[2]\n","\n","y_val_seq_flat = y_val_seq.reshape(num_val_seq * num_output_steps, num_sensors)\n","y_pred_scaled_flat = y_pred_scaled.reshape(num_val_seq * num_output_steps, num_sensors)\n","\n","y_val_true = scaler.inverse_transform(y_val_seq_flat)\n","y_val_pred = scaler.inverse_transform(y_pred_scaled_flat)\n","\n","# --- Evaluate model performance ---\n","print(\"\\nEvaluating model performance...\")\n","mse = mean_squared_error(y_val_true, y_val_pred)\n","rmse = np.sqrt(mse)\n","mae = np.mean(np.abs(y_val_true - y_val_pred))\n","r2 = r2_score(y_val_true, y_val_pred)\n","\n","print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n","print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n","print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n","print(f\"R-squared (R2) Score: {r2:.4f}\")"]},{"cell_type":"code","execution_count":7,"id":"hVIHM-2owJSU","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":140,"status":"ok","timestamp":1754815159594,"user":{"displayName":"DEEKSHITHA B","userId":"16421084222432259444"},"user_tz":-330},"id":"hVIHM-2owJSU","outputId":"5833e65f-f520-4929-a7f1-5c313ffad7eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Calculating prediction errors...\n","\n","Identifying anomalies...\n","Average prediction error: 2.56\n","Anomaly threshold (Mean + 3*Std): 2.56\n","Found 0 potential anomalies.\n","\n","Examples of detected anomalies:\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1115215057.py:30: DeprecationWarning: Calling nonzero on 0d arrays is deprecated, as it behaves surprisingly. Use `atleast_1d(cond).nonzero()` if the old behavior was intended. If the context of this warning is of the form `arr[nonzero(cond)]`, just use `arr[cond]`.\n","  anomaly_indices = np.where(average_prediction_error_per_window > anomaly_threshold)[0]\n"]}],"source":["import numpy as np\n","import pandas as pd\n","\n","# We need the original, un-sequenced data to map anomalies back to dates.\n","# We will assume we have 'df' from a previous cell.\n","# The 'df' DataFrame was created in Step 13 (Corrected).\n","\n","# --- Step 19: Calculate Prediction Errors ---\n","print(\"Calculating prediction errors...\")\n","# The errors are the absolute difference between the true and predicted values\n","prediction_errors = np.abs(y_val_true - y_val_pred)\n","\n","# We have a 2D array of errors. Let's find the average error for each prediction window\n","# The error for each 30-minute prediction window is the mean of all 6 steps and 325 sensors\n","average_prediction_error_per_window = np.mean(prediction_errors, axis=(1, 0)) # Average error for each sensor over the prediction window\n","\n","# --- Step 20: Identify Anomalies based on a threshold ---\n","print(\"\\nIdentifying anomalies...\")\n","\n","# We will use a simple statistical threshold.\n","# Anomaly threshold: 3 standard deviations above the mean error\n","error_mean = np.mean(average_prediction_error_per_window)\n","error_std = np.std(average_prediction_error_per_window)\n","anomaly_threshold = error_mean + (3 * error_std)\n","\n","print(f\"Average prediction error: {error_mean:.2f}\")\n","print(f\"Anomaly threshold (Mean + 3*Std): {anomaly_threshold:.2f}\")\n","\n","# Find the indices where the error exceeds the threshold\n","anomaly_indices = np.where(average_prediction_error_per_window > anomaly_threshold)[0]\n","print(f\"Found {len(anomaly_indices)} potential anomalies.\")\n","\n","# --- Step 21: Map anomalies back to dates and sensors ---\n","# The anomaly indices correspond to our validation sequences.\n","# We need to map them back to the original timestamps from our DataFrame.\n","val_dates = df.index[num_train_samples:]\n","\n","print(\"\\nExamples of detected anomalies:\")\n","for i in anomaly_indices[:5]: # Print the first 5 anomalies\n","    # The anomaly index 'i' corresponds to the sequence number.\n","    # We need to find the start date of that sequence.\n","    anomaly_start_date = val_dates[i + input_sequence_length]\n","\n","    # We can also find the sensor with the highest error in that anomaly window\n","    max_error_sensor_index = np.argmax(np.mean(prediction_errors[i], axis=0))\n","    max_error_sensor_id = f'sensor_{max_error_sensor_index}'\n","\n","    print(f\"Anomaly detected starting on: {anomaly_start_date.strftime('%Y-%m-%d %H:%M')}\")\n","    print(f\"  - Max error was on {max_error_sensor_id}\")"]},{"cell_type":"code","execution_count":8,"id":"f3vyGvxPwJU8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":340,"status":"ok","timestamp":1754815159936,"user":{"displayName":"DEEKSHITHA B","userId":"16421084222432259444"},"user_tz":-330},"id":"f3vyGvxPwJU8","outputId":"5561fa0c-168d-4458-8fb0-37810479722c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Calculating prediction errors...\n","\n","Identifying anomalies...\n","Total number of anomalous predictions found: 445154\n","\n","Examples of detected anomalies:\n","Anomaly detected on 2017-05-25 19:20\n","  - Sensor: sensor_17\n","  - True Speed: 63.50, Predicted Speed: 46.14\n","  - Error: 17.36 (above threshold)\n","Anomaly detected on 2017-05-25 19:20\n","  - Sensor: sensor_126\n","  - True Speed: 67.00, Predicted Speed: 55.07\n","  - Error: 11.93 (above threshold)\n","Anomaly detected on 2017-05-25 19:20\n","  - Sensor: sensor_301\n","  - True Speed: 31.80, Predicted Speed: 58.62\n","  - Error: 26.82 (above threshold)\n","Anomaly detected on 2017-05-25 19:20\n","  - Sensor: sensor_313\n","  - True Speed: 75.90, Predicted Speed: 61.65\n","  - Error: 14.25 (above threshold)\n","Anomaly detected on 2017-05-25 19:25\n","  - Sensor: sensor_17\n","  - True Speed: 64.40, Predicted Speed: 46.56\n","  - Error: 17.84 (above threshold)\n"]}],"source":["import numpy as np\n","import pandas as pd\n","\n","# We need the original, un-sequenced data to map anomalies back to dates.\n","# We will assume we have 'df' from a previous cell.\n","# The 'df' DataFrame was created in Step 13 (Corrected).\n","\n","# --- Step 19 (Corrected): Calculate Prediction Errors ---\n","print(\"Calculating prediction errors...\")\n","# The errors are the absolute difference between the true and predicted values\n","# These are already flattened to 2D, so we can't average by window this way.\n","prediction_errors = np.abs(y_val_true - y_val_pred)\n","\n","# --- Step 20 (Corrected): Identify Anomalies based on a threshold ---\n","print(\"\\nIdentifying anomalies...\")\n","\n","# Let's calculate the mean prediction error per sensor over all validation data.\n","average_prediction_error_per_sensor = np.mean(prediction_errors, axis=0)\n","\n","# We will define an anomaly as any single prediction error that is a certain\n","# number of standard deviations above the mean error for that specific sensor.\n","# This is a more robust approach than a single threshold for all predictions.\n","\n","# Calculate the mean and standard deviation for each sensor's error\n","error_mean_per_sensor = np.mean(prediction_errors, axis=0)\n","error_std_per_sensor = np.std(prediction_errors, axis=0)\n","\n","# Set the anomaly threshold per sensor\n","anomaly_threshold_per_sensor = error_mean_per_sensor + (3 * error_std_per_sensor)\n","\n","# Find the indices of all individual prediction errors that are anomalies\n","anomaly_mask = prediction_errors > anomaly_threshold_per_sensor\n","\n","# Get the row (prediction) and column (sensor) indices of anomalies\n","anomaly_indices = np.where(anomaly_mask)\n","\n","print(f\"Total number of anomalous predictions found: {len(anomaly_indices[0])}\")\n","\n","# --- Step 21 (Corrected): Map anomalies back to dates and sensors ---\n","# The anomaly indices correspond to our validation sequences.\n","# We need to map them back to the original timestamps from our DataFrame.\n","# Let's get the original dates for the validation set.\n","val_dates_flat = df.index[num_train_samples + input_sequence_length: num_train_samples + input_sequence_length + prediction_errors.shape[0]]\n","\n","print(\"\\nExamples of detected anomalies:\")\n","if len(anomaly_indices[0]) > 0:\n","    for i in range(min(5, len(anomaly_indices[0]))): # Print the first 5 anomalies\n","        time_step_index = anomaly_indices[0][i]\n","        sensor_index = anomaly_indices[1][i]\n","\n","        anomaly_date = val_dates_flat[time_step_index]\n","        anomaly_sensor = f'sensor_{sensor_index}'\n","\n","        true_value = y_val_true[time_step_index, sensor_index]\n","        predicted_value = y_val_pred[time_step_index, sensor_index]\n","        error_value = prediction_errors[time_step_index, sensor_index]\n","\n","        print(f\"Anomaly detected on {anomaly_date.strftime('%Y-%m-%d %H:%M')}\")\n","        print(f\"  - Sensor: {anomaly_sensor}\")\n","        print(f\"  - True Speed: {true_value:.2f}, Predicted Speed: {predicted_value:.2f}\")\n","        print(f\"  - Error: {error_value:.2f} (above threshold)\")\n","\n","else:\n","    print(\"No significant anomalies detected above the threshold.\")"]},{"cell_type":"code","execution_count":9,"id":"618dW70YyX1M","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":166167,"status":"ok","timestamp":1754815326105,"user":{"displayName":"DEEKSHITHA B","userId":"16421084222432259444"},"user_tz":-330},"id":"618dW70YyX1M","outputId":"2e165ffc-6e1d-45c6-9086-5380018272d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Creating DataFrame for Power BI...\n","Combined DataFrame created.\n","                 Date    Sensor  TrueSpeed  PredictedSpeed  PredictionError  \\\n","0 2017-05-25 19:20:00  sensor_0       71.1       69.919022         1.180978   \n","1 2017-05-25 19:20:00  sensor_1       61.8       54.053997         7.746003   \n","2 2017-05-25 19:20:00  sensor_2       63.7       59.482365         4.217635   \n","3 2017-05-25 19:20:00  sensor_3       68.2       61.040920         7.159080   \n","4 2017-05-25 19:20:00  sensor_4       68.1       61.700882         6.399118   \n","\n","   IsAnomaly  \n","0      False  \n","1      False  \n","2      False  \n","3      False  \n","4      False  \n","DataFrame shape: (20291700, 6)\n","\n","DataFrame successfully saved to /content/drive/MyDrive/smart_traffic_system/dashboard_data.csv\n","This file is now ready for import into Power BI.\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","\n","# --- Step D (Final): Create the DataFrame for Power BI from scratch ---\n","print(\"\\nCreating DataFrame for Power BI...\")\n","\n","# We need the flattened versions of our data\n","y_true_flat = y_val_true.flatten()\n","y_pred_flat = y_val_pred.flatten()\n","\n","# Calculate the total number of individual predictions\n","total_predictions = y_true_flat.shape[0]\n","\n","# --- Reconstruct the 'Date' and 'Sensor' columns ---\n","# The number of sensors is 325.\n","# The number of unique time steps is total_predictions / num_sensors\n","num_unique_time_steps = y_val_true.shape[0]\n","\n","# Get the date range for the validation predictions\n","start_val_date = df.index[num_train_samples + input_sequence_length]\n","val_dates_for_predictions = pd.date_range(\n","    start=start_val_date,\n","    periods=num_unique_time_steps,\n","    freq='5min'\n",")\n","\n","# Create the repeated date array\n","date_array = val_dates_for_predictions.repeat(y_val_true.shape[1])\n","\n","# Create the tiled sensor array\n","sensors_array = np.tile(df[target_columns].columns.values, num_unique_time_steps)\n","\n","# --- Build the DataFrame ---\n","powerbi_df = pd.DataFrame({\n","    'Date': date_array,\n","    'Sensor': sensors_array,\n","    'TrueSpeed': y_true_flat,\n","    'PredictedSpeed': y_pred_flat,\n","    'PredictionError': (y_true_flat - y_pred_flat),\n","})\n","\n","# --- Step E: Add IsAnomaly column ---\n","# We need to get the error stats per sensor from the un-flattened arrays.\n","prediction_errors_per_sensor = np.abs(y_val_true - y_val_pred)\n","\n","error_mean_per_sensor = np.mean(prediction_errors_per_sensor, axis=0)\n","error_std_per_sensor = np.std(prediction_errors_per_sensor, axis=0)\n","\n","anomaly_threshold_per_sensor = error_mean_per_sensor + (3 * error_std_per_sensor)\n","\n","anomaly_mask_flat = (np.abs(y_val_true - y_val_pred) > anomaly_threshold_per_sensor).flatten()\n","\n","powerbi_df['IsAnomaly'] = anomaly_mask_flat\n","\n","print(\"Combined DataFrame created.\")\n","print(powerbi_df.head())\n","print(f\"DataFrame shape: {powerbi_df.shape}\")\n","\n","# --- Step F: Save the DataFrame to a CSV file ---\n","output_file_path = os.path.join(base_path, 'dashboard_data.csv')\n","\n","try:\n","    powerbi_df.to_csv(output_file_path, index=False)\n","    print(f\"\\nDataFrame successfully saved to {output_file_path}\")\n","    print(\"This file is now ready for import into Power BI.\")\n","\n","except Exception as e:\n","    print(f\"An error occurred while saving the CSV file: {e}\")"]},{"cell_type":"code","execution_count":14,"id":"C5Cvw1o8yX49","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":0,"status":"ok","timestamp":1754815850557,"user":{"displayName":"DEEKSHITHA B","userId":"16421084222432259444"},"user_tz":-330},"id":"C5Cvw1o8yX49","outputId":"e29c2338-3ba0-4f07-a182-7ae9f3979e3b"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Loading all necessary data and model...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Making predictions and inverse transforming...\n","\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n","\n","Creating DataFrame for Power BI...\n","Combined DataFrame created.\n","                 Date    Sensor  TrueSpeed  PredictedSpeed  PredictionError  \\\n","0 2017-05-25 19:20:00  sensor_0       71.1       69.919022         1.180978   \n","1 2017-05-25 19:20:00  sensor_1       61.8       54.053997         7.746003   \n","2 2017-05-25 19:20:00  sensor_2       63.7       59.482365         4.217635   \n","3 2017-05-25 19:20:00  sensor_3       68.2       61.040920         7.159080   \n","4 2017-05-25 19:20:00  sensor_4       68.1       61.700882         6.399118   \n","\n","   IsAnomaly  \n","0      False  \n","1      False  \n","2      False  \n","3      False  \n","4      False  \n","DataFrame shape: (20291700, 6)\n","\n","Creating and saving a smaller DataFrame for Power BI...\n","\n","Smaller DataFrame successfully saved to /content/drive/MyDrive/smart_traffic_system/dashboard_data_small.csv\n","This file is now ready for import into Power BI.\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import h5py\n","import os\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow import keras\n","from tensorflow.keras.utils import custom_object_scope\n","\n","# --- Step A: Define the custom TransformerBlock class ---\n","# This is necessary for loading the model\n","class TransformerBlock(keras.layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n","        super().__init__(**kwargs)\n","        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = keras.Sequential(\n","            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = keras.layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.ff_dim = ff_dim\n","        self.rate = rate\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"ff_dim\": self.ff_dim,\n","            \"rate\": self.rate,\n","        })\n","        return config\n","\n","    def call(self, inputs):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","# --- Step B: Load all the necessary data and the model ---\n","print(\"Loading all necessary data and model...\")\n","base_path = '/content/drive/MyDrive/smart_traffic_system'\n","raw_file_path = os.path.join(base_path, 'pems-bay.h5')\n","model_path = os.path.join(base_path, 'transformer_model.h5')\n","\n","with h5py.File(raw_file_path, 'r') as hf:\n","    traffic_data = hf['speed']['block0_values'][:]\n","\n","start_date = '2017-01-01 00:00:00'\n","num_intervals = traffic_data.shape[0]\n","date_range = pd.date_range(start=start_date, periods=num_intervals, freq='5min')\n","num_sensors = traffic_data.shape[1]\n","sensor_columns = [f'sensor_{i}' for i in range(num_sensors)]\n","\n","df = pd.DataFrame(traffic_data, index=date_range, columns=sensor_columns)\n","\n","target_columns = [col for col in df.columns if col.startswith('sensor_')]\n","target_data = df[target_columns].values\n","\n","train_split = 0.8\n","num_samples = df.shape[0]\n","num_train_samples = int(num_samples * train_split)\n","\n","X_train_targets = target_data[:num_train_samples]\n","X_val_targets = target_data[num_train_samples:]\n","\n","scaler = MinMaxScaler()\n","scaler.fit(X_train_targets)\n","X_val_targets_scaled = scaler.transform(X_val_targets)\n","\n","input_sequence_length = 12\n","output_sequence_length = 6\n","\n","def create_sequences(data, input_steps, output_steps):\n","    X, y = [], []\n","    for i in range(len(data) - input_steps - output_steps):\n","        X.append(data[i:(i + input_steps)])\n","        y.append(data[(i + input_steps):(i + input_steps + output_steps)])\n","    return np.array(X), np.array(y)\n","\n","X_val_seq, y_val_seq = create_sequences(X_val_targets_scaled, input_sequence_length, output_sequence_length)\n","\n","with custom_object_scope({'TransformerBlock': TransformerBlock}):\n","    model = keras.models.load_model(model_path)\n","\n","# --- Step C: Make predictions and inverse transform ---\n","print(\"Making predictions and inverse transforming...\")\n","y_pred_scaled = model.predict(X_val_seq)\n","\n","y_val_true = scaler.inverse_transform(y_val_seq.reshape(-1, num_sensors))\n","y_val_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1, num_sensors))\n","\n","# --- Step D: Create the DataFrame for Power BI ---\n","print(\"\\nCreating DataFrame for Power BI...\")\n","\n","# Reconstruct the Date column\n","start_val_date_for_predictions = df.index[num_train_samples + input_sequence_length]\n","num_val_sequences = X_val_seq.shape[0]\n","total_prediction_steps = num_val_sequences * output_sequence_length\n","\n","val_dates_for_predictions = pd.date_range(\n","    start=start_val_date_for_predictions,\n","    periods=total_prediction_steps,\n","    freq='5min'\n",")\n","\n","# Reconstruct the Sensor column\n","sensors_array = np.tile(df[target_columns].columns.values, total_prediction_steps)\n","\n","# Correct the date array length by repeating each date for each sensor\n","date_array_repeated = val_dates_for_predictions.repeat(num_sensors)\n","\n","# Create the DataFrame\n","powerbi_df = pd.DataFrame({\n","    'Date': date_array_repeated,\n","    'Sensor': np.tile(df[target_columns].columns.values, total_prediction_steps),\n","    'TrueSpeed': y_val_true.flatten(),\n","    'PredictedSpeed': y_val_pred.flatten(),\n","    'PredictionError': (y_val_true - y_val_pred).flatten(),\n","})\n","\n","# --- Step E: Add IsAnomaly column ---\n","prediction_errors_per_sensor = np.abs(y_val_true - y_val_pred)\n","\n","error_mean_per_sensor = np.mean(prediction_errors_per_sensor, axis=0)\n","error_std_per_sensor = np.std(prediction_errors_per_sensor, axis=0)\n","\n","anomaly_threshold_per_sensor = error_mean_per_sensor + (3 * error_std_per_sensor)\n","\n","anomaly_mask_flat = (np.abs(y_val_true - y_val_pred) > anomaly_threshold_per_sensor).flatten()\n","\n","powerbi_df['IsAnomaly'] = anomaly_mask_flat\n","\n","print(\"Combined DataFrame created.\")\n","print(powerbi_df.head())\n","print(f\"DataFrame shape: {powerbi_df.shape}\")\n","\n","# --- Step F: Create and save the smaller DataFrame ---\n","print(\"\\nCreating and saving a smaller DataFrame for Power BI...\")\n","\n","start_date = '2017-05-01'\n","end_date = '2017-05-31'\n","\n","smaller_powerbi_df = powerbi_df[\n","    (powerbi_df['Date'] >= start_date) & (powerbi_df['Date'] <= end_date)\n","].copy()\n","\n","output_file_path = os.path.join(base_path, 'dashboard_data_small.csv')\n","\n","try:\n","    smaller_powerbi_df.to_csv(output_file_path, index=False)\n","    print(f\"\\nSmaller DataFrame successfully saved to {output_file_path}\")\n","    print(\"This file is now ready for import into Power BI.\")\n","\n","except Exception as e:\n","    print(f\"An error occurred while saving the CSV file: {e}\")"]},{"cell_type":"code","execution_count":null,"id":"3-tJZ5rJyYAC","metadata":{"executionInfo":{"elapsed":662360,"status":"aborted","timestamp":1754815340493,"user":{"displayName":"DEEKSHITHA B","userId":"16421084222432259444"},"user_tz":-330},"id":"3-tJZ5rJyYAC"},"outputs":[],"source":["!ls \"/content/drive/MyDrive/smart_traffic_system\""]},{"cell_type":"code","execution_count":null,"id":"jjljzch3yYJI","metadata":{"executionInfo":{"elapsed":662429,"status":"aborted","timestamp":1754815340564,"user":{"displayName":"DEEKSHITHA B","userId":"16421084222432259444"},"user_tz":-330},"id":"jjljzch3yYJI"},"outputs":[],"source":["!ls -R \"/content/drive/MyDrive/smart_traffic_system\""]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.23"}},"nbformat":4,"nbformat_minor":5}
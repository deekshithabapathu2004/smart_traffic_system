{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3096dff3-fe2f-45d9-8acc-ab2e6e77e7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: ..\\data\\raw\\pems-bay.h5\n",
      "Loaded traffic data with shape: (52116, 325)\n",
      "Created a date range from 2017-01-01 00:00:00 to 2017-06-30 22:55:00\n",
      "\n",
      "DataFrame created successfully:\n",
      "                     sensor_0  sensor_1  sensor_2  sensor_3  sensor_4  \\\n",
      "2017-01-01 00:00:00      71.4      67.8      70.5      67.4      68.8   \n",
      "2017-01-01 00:05:00      71.6      67.5      70.6      67.5      68.7   \n",
      "2017-01-01 00:10:00      71.6      67.6      70.2      67.4      68.7   \n",
      "2017-01-01 00:15:00      71.1      67.5      70.3      68.0      68.5   \n",
      "2017-01-01 00:20:00      71.7      67.8      70.2      68.1      68.4   \n",
      "\n",
      "                     sensor_5  sensor_6  sensor_7  sensor_8  sensor_9  ...  \\\n",
      "2017-01-01 00:00:00      66.6      66.8      68.0      66.8      69.0  ...   \n",
      "2017-01-01 00:05:00      66.6      66.8      67.8      66.5      68.2  ...   \n",
      "2017-01-01 00:10:00      66.1      66.8      67.8      66.2      67.8  ...   \n",
      "2017-01-01 00:15:00      66.7      66.6      67.7      65.9      67.8  ...   \n",
      "2017-01-01 00:20:00      66.9      66.1      67.7      66.1      67.8  ...   \n",
      "\n",
      "                     sensor_315  sensor_316  sensor_317  sensor_318  \\\n",
      "2017-01-01 00:00:00        68.8        67.9        68.8        68.0   \n",
      "2017-01-01 00:05:00        68.4        67.3        68.4        67.6   \n",
      "2017-01-01 00:10:00        68.4        67.4        68.4        67.5   \n",
      "2017-01-01 00:15:00        68.5        67.5        68.5        67.5   \n",
      "2017-01-01 00:20:00        68.5        67.7        68.5        67.4   \n",
      "\n",
      "                     sensor_319  sensor_320  sensor_321  sensor_322  \\\n",
      "2017-01-01 00:00:00        69.2        68.9        70.4        68.8   \n",
      "2017-01-01 00:05:00        70.4        68.8        70.1        68.4   \n",
      "2017-01-01 00:10:00        70.2        68.3        69.8        68.4   \n",
      "2017-01-01 00:15:00        70.4        68.7        70.2        68.4   \n",
      "2017-01-01 00:20:00        69.6        69.1        70.0        68.4   \n",
      "\n",
      "                     sensor_323  sensor_324  \n",
      "2017-01-01 00:00:00        71.1        68.0  \n",
      "2017-01-01 00:05:00        70.8        67.4  \n",
      "2017-01-01 00:10:00        70.5        67.9  \n",
      "2017-01-01 00:15:00        70.8        67.6  \n",
      "2017-01-01 00:20:00        71.0        67.9  \n",
      "\n",
      "[5 rows x 325 columns]\n",
      "\n",
      "DataFrame shape: (52116, 325)\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 52116 entries, 2017-01-01 00:00:00 to 2017-06-30 22:55:00\n",
      "Freq: 5min\n",
      "Columns: 325 entries, sensor_0 to sensor_324\n",
      "dtypes: float64(325)\n",
      "memory usage: 129.6 MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "# --- Step 1: Load the raw data from the .h5 file ---\n",
    "file_path = os.path.join('..', 'data', 'raw', 'pems-bay.h5')\n",
    "print(f\"Loading data from: {file_path}\")\n",
    "\n",
    "try:\n",
    "    with h5py.File(file_path, 'r') as hf:\n",
    "        traffic_data = hf['speed']['block0_values'][:]\n",
    "        # The [:] loads the entire HDF5 dataset into memory as a NumPy array\n",
    "        print(f\"Loaded traffic data with shape: {traffic_data.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data loading: {e}\")\n",
    "    traffic_data = None # Set to None so the rest of the code doesn't fail\n",
    "\n",
    "if traffic_data is not None:\n",
    "    # --- Step 2: Create a time index for the DataFrame ---\n",
    "    # The data starts on Jan 1, 2017, at 5-minute intervals.\n",
    "    # This is a key piece of information we need to know about the dataset.\n",
    "    start_date = '2017-01-01 00:00:00'\n",
    "    num_intervals = traffic_data.shape[0]\n",
    "\n",
    "    # Use pandas to create a datetime index\n",
    "    date_range = pd.date_range(start=start_date, periods=num_intervals, freq='5min')\n",
    "    print(f\"Created a date range from {date_range[0]} to {date_range[-1]}\")\n",
    "\n",
    "    # --- Step 3: Create a Pandas DataFrame ---\n",
    "    # Each column in the DataFrame corresponds to a sensor.\n",
    "    num_sensors = traffic_data.shape[1]\n",
    "    sensor_columns = [f'sensor_{i}' for i in range(num_sensors)]\n",
    "\n",
    "    df = pd.DataFrame(traffic_data, index=date_range, columns=sensor_columns)\n",
    "\n",
    "    print(\"\\nDataFrame created successfully:\")\n",
    "    print(df.head())\n",
    "    print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "    print(f\"DataFrame info:\")\n",
    "    df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b79e5f2f-5ebf-4716-bbdb-614d997b2460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for missing values...\n",
      "Total number of missing values: 0\n",
      "\n",
      "Adding time-based features...\n",
      "New features added. Here's the updated DataFrame with new columns:\n",
      "                     sensor_0  sensor_1  sensor_2  sensor_3  sensor_4  \\\n",
      "2017-01-01 00:00:00      71.4      67.8      70.5      67.4      68.8   \n",
      "2017-01-01 00:05:00      71.6      67.5      70.6      67.5      68.7   \n",
      "2017-01-01 00:10:00      71.6      67.6      70.2      67.4      68.7   \n",
      "2017-01-01 00:15:00      71.1      67.5      70.3      68.0      68.5   \n",
      "2017-01-01 00:20:00      71.7      67.8      70.2      68.1      68.4   \n",
      "\n",
      "                     sensor_5  sensor_6  sensor_7  sensor_8  sensor_9  ...  \\\n",
      "2017-01-01 00:00:00      66.6      66.8      68.0      66.8      69.0  ...   \n",
      "2017-01-01 00:05:00      66.6      66.8      67.8      66.5      68.2  ...   \n",
      "2017-01-01 00:10:00      66.1      66.8      67.8      66.2      67.8  ...   \n",
      "2017-01-01 00:15:00      66.7      66.6      67.7      65.9      67.8  ...   \n",
      "2017-01-01 00:20:00      66.9      66.1      67.7      66.1      67.8  ...   \n",
      "\n",
      "                     sensor_320  sensor_321  sensor_322  sensor_323  \\\n",
      "2017-01-01 00:00:00        68.9        70.4        68.8        71.1   \n",
      "2017-01-01 00:05:00        68.8        70.1        68.4        70.8   \n",
      "2017-01-01 00:10:00        68.3        69.8        68.4        70.5   \n",
      "2017-01-01 00:15:00        68.7        70.2        68.4        70.8   \n",
      "2017-01-01 00:20:00        69.1        70.0        68.4        71.0   \n",
      "\n",
      "                     sensor_324  dayofweek  hour  minute  dayofyear  \\\n",
      "2017-01-01 00:00:00        68.0          6     0       0          1   \n",
      "2017-01-01 00:05:00        67.4          6     0       5          1   \n",
      "2017-01-01 00:10:00        67.9          6     0      10          1   \n",
      "2017-01-01 00:15:00        67.6          6     0      15          1   \n",
      "2017-01-01 00:20:00        67.9          6     0      20          1   \n",
      "\n",
      "                     weekofyear  \n",
      "2017-01-01 00:00:00          52  \n",
      "2017-01-01 00:05:00          52  \n",
      "2017-01-01 00:10:00          52  \n",
      "2017-01-01 00:15:00          52  \n",
      "2017-01-01 00:20:00          52  \n",
      "\n",
      "[5 rows x 330 columns]\n",
      "\n",
      "DataFrame columns:\n",
      "Index(['sensor_0', 'sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5',\n",
      "       'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9',\n",
      "       ...\n",
      "       'sensor_320', 'sensor_321', 'sensor_322', 'sensor_323', 'sensor_324',\n",
      "       'dayofweek', 'hour', 'minute', 'dayofyear', 'weekofyear'],\n",
      "      dtype='object', length=330)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Check for missing values ---\n",
    "print(\"Checking for missing values...\")\n",
    "missing_values = df.isnull().sum().sum()\n",
    "print(f\"Total number of missing values: {missing_values}\")\n",
    "\n",
    "# Note: For this specific dataset, the raw data is often clean, but in a real-world project,\n",
    "# you would handle missing values here (e.g., with interpolation, mean imputation, etc.).\n",
    "# Since the sum is likely 0, we can proceed.\n",
    "\n",
    "\n",
    "# --- Step 5: Add time-based features ---\n",
    "print(\"\\nAdding time-based features...\")\n",
    "\n",
    "# Extracting features from the DataFrame's index\n",
    "df['dayofweek'] = df.index.dayofweek # Monday is 0, Sunday is 6\n",
    "df['hour'] = df.index.hour\n",
    "df['minute'] = df.index.minute\n",
    "df['dayofyear'] = df.index.dayofyear\n",
    "df['weekofyear'] = df.index.isocalendar().week # Use isocalendar for ISO week number\n",
    "\n",
    "print(\"New features added. Here's the updated DataFrame with new columns:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataFrame columns:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a98f09fb-721b-4a58-9ebc-0d4760e14a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving the processed DataFrame...\n",
      "DataFrame successfully saved to ..\\data\\processed\\traffic_data_processed.parquet\n",
      "Verification successful. Loaded DataFrame head:\n",
      "                     sensor_0  sensor_1  sensor_2  sensor_3  sensor_4  \\\n",
      "2017-01-01 00:00:00      71.4      67.8      70.5      67.4      68.8   \n",
      "2017-01-01 00:05:00      71.6      67.5      70.6      67.5      68.7   \n",
      "2017-01-01 00:10:00      71.6      67.6      70.2      67.4      68.7   \n",
      "2017-01-01 00:15:00      71.1      67.5      70.3      68.0      68.5   \n",
      "2017-01-01 00:20:00      71.7      67.8      70.2      68.1      68.4   \n",
      "\n",
      "                     sensor_5  sensor_6  sensor_7  sensor_8  sensor_9  ...  \\\n",
      "2017-01-01 00:00:00      66.6      66.8      68.0      66.8      69.0  ...   \n",
      "2017-01-01 00:05:00      66.6      66.8      67.8      66.5      68.2  ...   \n",
      "2017-01-01 00:10:00      66.1      66.8      67.8      66.2      67.8  ...   \n",
      "2017-01-01 00:15:00      66.7      66.6      67.7      65.9      67.8  ...   \n",
      "2017-01-01 00:20:00      66.9      66.1      67.7      66.1      67.8  ...   \n",
      "\n",
      "                     sensor_320  sensor_321  sensor_322  sensor_323  \\\n",
      "2017-01-01 00:00:00        68.9        70.4        68.8        71.1   \n",
      "2017-01-01 00:05:00        68.8        70.1        68.4        70.8   \n",
      "2017-01-01 00:10:00        68.3        69.8        68.4        70.5   \n",
      "2017-01-01 00:15:00        68.7        70.2        68.4        70.8   \n",
      "2017-01-01 00:20:00        69.1        70.0        68.4        71.0   \n",
      "\n",
      "                     sensor_324  dayofweek  hour  minute  dayofyear  \\\n",
      "2017-01-01 00:00:00        68.0          6     0       0          1   \n",
      "2017-01-01 00:05:00        67.4          6     0       5          1   \n",
      "2017-01-01 00:10:00        67.9          6     0      10          1   \n",
      "2017-01-01 00:15:00        67.6          6     0      15          1   \n",
      "2017-01-01 00:20:00        67.9          6     0      20          1   \n",
      "\n",
      "                     weekofyear  \n",
      "2017-01-01 00:00:00          52  \n",
      "2017-01-01 00:05:00          52  \n",
      "2017-01-01 00:10:00          52  \n",
      "2017-01-01 00:15:00          52  \n",
      "2017-01-01 00:20:00          52  \n",
      "\n",
      "[5 rows x 330 columns]\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6: Install pyarrow if not already installed ---\n",
    "# This is needed to save a DataFrame to a Parquet file.\n",
    "# If you get a ModuleNotFoundError, uncomment the line below and run it.\n",
    "# Then, comment it again and rerun the cell.\n",
    "# !pip install pyarrow fastparquet\n",
    "\n",
    "# --- Step 7: Save the processed DataFrame to a file ---\n",
    "print(\"\\nSaving the processed DataFrame...\")\n",
    "\n",
    "# Define the output file path in the 'data/processed' folder\n",
    "output_file_path = os.path.join('..', 'data', 'processed', 'traffic_data_processed.parquet')\n",
    "\n",
    "try:\n",
    "    # Save the DataFrame to a Parquet file\n",
    "    df.to_parquet(output_file_path, index=True)\n",
    "    print(f\"DataFrame successfully saved to {output_file_path}\")\n",
    "\n",
    "    # Optional: Verify by loading it back\n",
    "    loaded_df = pd.read_parquet(output_file_path)\n",
    "    print(\"Verification successful. Loaded DataFrame head:\")\n",
    "    print(loaded_df.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving or loading the Parquet file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc89a7a3-d88a-4366-a3c6-5651b2976782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the processed DataFrame...\n",
      "DataFrame loaded with shape: (52116, 330)\n",
      "Total samples: 52116\n",
      "Training samples: 41692\n",
      "Validation samples: 10424\n",
      "\n",
      "Data splitting complete.\n",
      "X_train_targets shape: (41692, 325)\n",
      "X_val_targets shape: (10424, 325)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Step 8: Load the processed data ---\n",
    "print(\"Loading the processed DataFrame...\")\n",
    "processed_file_path = os.path.join('..', 'data', 'processed', 'traffic_data_processed.parquet')\n",
    "df = pd.read_parquet(processed_file_path)\n",
    "print(f\"DataFrame loaded with shape: {df.shape}\")\n",
    "\n",
    "# Separate the target sensors (traffic speeds) from the exogenous features (time-based features)\n",
    "target_columns = [col for col in df.columns if col.startswith('sensor_')]\n",
    "feature_columns = ['dayofweek', 'hour', 'minute', 'dayofyear', 'weekofyear']\n",
    "\n",
    "target_data = df[target_columns].values\n",
    "feature_data = df[feature_columns].values\n",
    "\n",
    "# --- Step 9: Split the data into training and validation sets ---\n",
    "# We will use the first 80% of the data for training and the last 20% for validation.\n",
    "# It's important to keep the time order for time-series data.\n",
    "train_split = 0.8\n",
    "num_samples = df.shape[0]\n",
    "num_train_samples = int(num_samples * train_split)\n",
    "num_val_samples = num_samples - num_train_samples\n",
    "\n",
    "print(f\"Total samples: {num_samples}\")\n",
    "print(f\"Training samples: {num_train_samples}\")\n",
    "print(f\"Validation samples: {num_val_samples}\")\n",
    "\n",
    "X_train_targets = target_data[:num_train_samples]\n",
    "X_train_features = feature_data[:num_train_samples]\n",
    "\n",
    "X_val_targets = target_data[num_train_samples:]\n",
    "X_val_features = feature_data[num_train_samples:]\n",
    "\n",
    "# We will create sequences in a later step. For now, this split is a good first step.\n",
    "\n",
    "print(\"\\nData splitting complete.\")\n",
    "print(f\"X_train_targets shape: {X_train_targets.shape}\")\n",
    "print(f\"X_val_targets shape: {X_val_targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3eca053-72c8-4adf-82a6-f7e7c896c68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing the target data...\n",
      "Data successfully normalized.\n",
      "X_train_targets_scaled mean: 0.8229, std: 0.1384\n",
      "X_val_targets_scaled mean: 0.8198, std: 0.1405\n",
      "\n",
      "Sequential data creation complete.\n",
      "Training input sequences shape: (41674, 12, 325)\n",
      "Training output sequences shape: (41674, 6, 325)\n",
      "Validation input sequences shape: (10406, 12, 325)\n",
      "Validation output sequences shape: (10406, 6, 325)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# --- Step 10: Normalize the target data using MinMaxScaler ---\n",
    "print(\"Normalizing the target data...\")\n",
    "\n",
    "# We fit the scaler ONLY on the training data to prevent data leakage.\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_targets)\n",
    "\n",
    "# Now we transform both the training and validation data.\n",
    "X_train_targets_scaled = scaler.transform(X_train_targets)\n",
    "X_val_targets_scaled = scaler.transform(X_val_targets)\n",
    "\n",
    "print(\"Data successfully normalized.\")\n",
    "print(f\"X_train_targets_scaled mean: {X_train_targets_scaled.mean():.4f}, std: {X_train_targets_scaled.std():.4f}\")\n",
    "print(f\"X_val_targets_scaled mean: {X_val_targets_scaled.mean():.4f}, std: {X_val_targets_scaled.std():.4f}\")\n",
    "\n",
    "# For the features (day of week, etc.), they are already small integer values\n",
    "# and we can handle them with embeddings in our model, so we won't scale them for now.\n",
    "\n",
    "# --- Step 11: Create the sequential data format for the Transformer ---\n",
    "def create_sequences(data, input_steps, output_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - input_steps - output_steps):\n",
    "        X.append(data[i:(i + input_steps)])\n",
    "        y.append(data[(i + input_steps):(i + input_steps + output_steps)])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Let's define the sequence lengths for our Transformer\n",
    "input_sequence_length = 12 # 1 hour of history (12 * 5-minute intervals)\n",
    "output_sequence_length = 6 # 30 minutes to predict (6 * 5-minute intervals)\n",
    "\n",
    "# We will use the scaled targets to create our sequences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_targets_scaled, input_sequence_length, output_sequence_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_targets_scaled, input_sequence_length, output_sequence_length)\n",
    "\n",
    "print(\"\\nSequential data creation complete.\")\n",
    "print(f\"Training input sequences shape: {X_train_seq.shape}\")\n",
    "print(f\"Training output sequences shape: {y_train_seq.shape}\")\n",
    "print(f\"Validation input sequences shape: {X_val_seq.shape}\")\n",
    "print(f\"Validation output sequences shape: {y_val_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1bdce4-f3e7-4479-87ed-7351c749cff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
